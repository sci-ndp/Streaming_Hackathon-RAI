{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b6d4f5-e7eb-40bd-82f5-723081d80802",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <img src=\"https://scidx.sci.utah.edu/wp-content/uploads/2024/12/logo-sm.png\" alt=\"scidx Logo\"/>\n",
    "        <img src=\"https://nationaldataplatform.org/National_Data_Platform_horiz_stacked.svg\" alt=\"NDP Logo\" width=\"400\" style=\"padding-left:100px\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb0d25",
   "metadata": {},
   "source": [
    "# SciDX Streaming Capabilities Demonstration \n",
    "\n",
    "This demonstration showcases the **SciDX Streaming capabilities**, leveraging both the **SciDX NDP Endpoint Library** for managing data objects and the **Streaming Library** for real-time data streaming. <br>\n",
    "\n",
    "**`SciDX NDP Endpoint Library`:** Used to register and discover data objects (acts as the Data Provider). Interacts with the NDP endpoint API to register and manage data objects. <br>\n",
    "**`Streaming Library`:** Used to create, manage, and consume real-time data streams (acts as the Data Consumer). Manages real-time data streams, including applying filters and consuming messages.\n",
    "\n",
    "The following diagram illustrates the interaction between the data provider (who registers the stream) and the data consumer (who subscribes to the filtered stream):\n",
    "\n",
    "![Data Stream Library](rsc/data_stream_library.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba25b4c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## **Setup**: Client preparation\n",
    "\n",
    "1. **Import necessary modules** for handling data streams:\n",
    "\n",
    "   a. Import `StreamingClient` from `scidx_streaming` module for handling streaming functionality <br>\n",
    "   b. Import `APIClient` from `pointofpresence` module for API interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scidx_streaming import StreamingClient\n",
    "from pointofpresence import APIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba1cf5",
   "metadata": {},
   "source": [
    "2. **API Authentication Token Setup**\n",
    "\n",
    "    1. Navigate to https://token.ndp.utah.edu\n",
    "    \n",
    "    2. If not already authenticated:\n",
    "       - Select the `CILogon` button\n",
    "       - Choose your institution from the Identity Provider list\n",
    "       - Complete the institutional login process\n",
    "    \n",
    "    3. Upon successful authentication, you will be redirected to token.ndp.utah.edu\n",
    "    \n",
    "    4. Locate the `Access Token` field and copy the token value\n",
    "    \n",
    "    5. Replace `<your_token>` in the configuration below with your copied access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=\"<your_token>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8a7d6-3974-4880-acb7-846dbf8d3375",
   "metadata": {},
   "source": [
    "3. **Initialize the API or Kafka client** to register and discover data streams.\n",
    "\n",
    "    a. `APIClient`: handle data registration and discovery. <br>\n",
    "    b. `StreamingClient`: handle real-time data streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdbdd01-e195-4f3b-b17e-45771538deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL of the API Client to connect to the NDP endpoint service\n",
    "API_URL=\"155.101.6.191:8003\"\n",
    "\n",
    "# Initialize the NDP endpoint client for data registration and discovery\n",
    "client = APIClient(base_url=API_URL, token=TOKEN)\n",
    "\n",
    "# Initialize the Streaming client for real-time data streaming\n",
    "streaming = StreamingClient(client)\n",
    "print(f\"Streaming Client initialized. User ID: {streaming.user_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e71191",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## **Basic Usage**\n",
    "\n",
    "**Data Provider**: Register data stream \n",
    "\n",
    "1. Registering the data source metadata.\n",
    "2. Verifying its discoverability.\n",
    "\n",
    "> **Note:**  \n",
    "> SciDX provides a unified method for registering both API-based and static file-based streams using **`client.register_url(metadata)`**.  \n",
    "> - **Static files**: Pre-existing datasets hosted on a URL (`CSV`, `NetCDF`, `TXT`, `JSON`). Example: https://horel.chpc.utah.edu/data/meop/level3/ebus_2021/ebus_min_2021_09.csv\n",
    "> - **API streams**: External APIs providing real-time or batch data. Example: https://data.sagecontinuum.org/api/v0/stream?name=wxt.wind.direction\n",
    "\n",
    "> Each registration requires defining `metadata`, which includes:  \n",
    "> - Basic information: Name, title, and organization ID.  \n",
    "> - Source details: Specific configurations related to the data source.  \n",
    "> - Mapping: Selecting and renaming relevant fields.  \n",
    "> - Processing rules: Extracting data from structured responses.\n",
    "\n",
    "**Data Consumer**:\n",
    "1. Discover and apply filters(optional) to the registered data sources to create a custom data stream.\n",
    "2. Subscribe to and consume the custom data stream in real-time.\n",
    "3. Process and visualize the incoming data dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412e005",
   "metadata": {},
   "source": [
    "\n",
    "### **Static Files Overview**\n",
    "`SciDX` allows users to register data streams from `Static Files`, as pre-existing datasets stored in structured file formats: \n",
    "- **CSV** (Comma-Separated Values) \n",
    "- **NetCDF** (Common scientific data format) \n",
    "- **TXT** (Structured text files) \n",
    "- **JSON** (JavaScript Object Notation) \n",
    "\n",
    "These files can be registered into the SciDX system to enable real-time consumption and transformation via Kafka streams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf287e0",
   "metadata": {},
   "source": [
    "#### **CSV File Example**\n",
    "\n",
    "#### **1.1** Register a **`CSV`** file\n",
    "\n",
    "In this step, we will use the **NDP endpoint client**, and the metadata for resgitsering an online **CSV** into our NDP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d047f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the CSV data with the NDP endpoint client\n",
    "csv_metadata = {\n",
    "    \"resource_name\": \"csv_example_meop\",\n",
    "    \"resource_title\": \"Example CSV\",\n",
    "    \"owner_org\": \"saleem_test\",\n",
    "    \"resource_url\": \"https://horel.chpc.utah.edu/data/meop/level3/ebus_2021/ebus_min_2021_09.csv\",\n",
    "    \"file_type\": \"CSV\",\n",
    "    \"notes\": \"Some additional notes about the resource.\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(client.register_url(csv_metadata))\n",
    "    print('Correctly registered')\n",
    "except ValueError as e: # If the dataset already exists just show the error\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507a9d1",
   "metadata": {},
   "source": [
    "#### **1.2** Search for the registered entry\n",
    "\n",
    "Now that we have registered data source, we will: \n",
    "1. Use the **NDP endpoint client** to search for datasets using the `search_datasets` method.\n",
    "2. Verify that the **registered data object** is correctly stored and available for discovery.\n",
    "3. Confirm meta data accuracy before data consumption.\n",
    "\n",
    "This ensures the dataset is discoverable for use by the Data Consumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2a4464",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Search for the registered CSV metadata\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39msearch_datasets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv_example_meop\u001b[39m\u001b[38;5;124m\"\u001b[39m, server\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of datasets found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(search_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Search for the registered CSV metadata\n",
    "search_results = client.search_datasets(\"csv_example_meop\", server=\"local\")\n",
    "print(f\"Number of datasets found: {len(search_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dbc1b3",
   "metadata": {},
   "source": [
    "#### **1.3** Create a Data Stream from the registered entry\n",
    "\n",
    "Now we will leverage the functionalities of `Streaming Client` to consume the data stream registered by `API Client`.\n",
    "\n",
    "The `create_kafka_stream` function searches for datasets matching the provided keywords, applies filtering semantics, and creates a real-time Kafka stream for consumption.\n",
    "\n",
    "Function parameters\n",
    "- **`keywords`**: List of keywords to filter relevant datasets.\n",
    "- filter_semantics: Optional, defines filtering criteria for datasets.\n",
    "- match_all: Optional, if True only data sources with all the keywords will be selected.\n",
    "- username: Optional username for authentication in protected data sources.\n",
    "- password: Optional password for authentication in protected data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9488e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kafka stream for the registered CSV data\n",
    "stream = await streaming.create_kafka_stream(\n",
    "    keywords=[\"csv_example_meop\"]\n",
    ")\n",
    "\n",
    "# Retrieve the stream's topic name\n",
    "topic = stream.data_stream_id\n",
    "print(f\"Stream created: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e9786",
   "metadata": {},
   "source": [
    "#### **1.4** Consume the Streamed Data \n",
    "\n",
    "Now that we have successfully created a Kafka data stream, we transition to real-time data consumption. This step involves:\n",
    "\n",
    "1. Initializing a kafka consumer: Passing the data stream topic to the consume_kafka_messages function.\n",
    "2. Listening for incoming messages: Continuously receiving new filtered data in real-time.\n",
    "3. Processing and updating the data dynamically: Messages are appended to a DataFrame for analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start consuming the filtered Kafka stream\n",
    "consumer = streaming.consume_kafka_messages(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "consumer.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece8651",
   "metadata": {},
   "source": [
    "#### **1.5** Stop Data Consumption and Clean up \n",
    "\n",
    "To wrap up, we will: \n",
    "1. Stop the data consumer to halt data processing.\n",
    "2. Delete the created stream from the Kafka topic using the Streaming client.\n",
    "3. Remove the registered dataset using the NDP endpoint client.\n",
    "\n",
    "This ensures all resources and background tasks are properly released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Kafka consumer\n",
    "consumer.stop()\n",
    "\n",
    "# Delete the Kafka stream\n",
    "await streaming.delete_stream(stream)\n",
    "\n",
    "# Delete the registered dataset from the NDP endpoint system\n",
    "client.delete_resource_by_id(search_results[0][\"id\"])\n",
    "print(\"Cleanup completed: Stream and registered dataset deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306378b9",
   "metadata": {},
   "source": [
    "#### **NetCDF File Example**\n",
    "\n",
    "#### **2.1** Register a **`NetCDF`** file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the NetCDF data with the NDP endpoint client\n",
    "netcdf_metadata = {\n",
    "    \"resource_name\": \"netcdf_example_noaa\",\n",
    "    \"resource_title\": \"Example NetCDF\",\n",
    "    \"owner_org\": \"saleem_test\",\n",
    "    \"resource_url\": \"https://noaa-nesdis-tcprimed-pds.s3.amazonaws.com/v01r01/final/1987/AL/02/TCPRIMED_v01r01-final_AL021987_SSMI_F08_000766_19870813094358.nc\",\n",
    "    \"file_type\": \"NetCDF\",\n",
    "    \"notes\": \"Some additional notes about the resource.\",\n",
    "    \"processing\": {\n",
    "        \"group\": \"overpass_metadata\"\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(client.register_url(netcdf_metadata))\n",
    "    print('Correctly registered')\n",
    "except ValueError as e: # If the dataset already exists just show the error\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8bea1f",
   "metadata": {},
   "source": [
    "#### **2.2** Search for the registered entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the registered Earthscope data stream\n",
    "search_results = client.search_datasets(\"netcdf_example_noaa\", server=\"local\")\n",
    "print(f\"Number of datasets found: {len(search_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ef87b",
   "metadata": {},
   "source": [
    "#### **2.3** Create a Data Stream from the registered entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kafka stream without filters\n",
    "stream = await streaming.create_kafka_stream(\n",
    "    keywords=[\"netcdf_example_noaa\"]\n",
    ")\n",
    "\n",
    "# Retrieve the stream's topic name\n",
    "topic = stream.data_stream_id\n",
    "print(f\"Stream created: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4410868",
   "metadata": {},
   "source": [
    "#### **2.4** Consume the Streamed Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e5cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start consuming the filtered Kafka stream\n",
    "consumer = streaming.consume_kafka_messages(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7fb7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After some seconds you can visualize the dataset\n",
    "consumer.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde6d74",
   "metadata": {},
   "source": [
    "#### **2.5** Stop Data Consumption and Clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a890b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Kafka consumer\n",
    "consumer.stop()\n",
    "\n",
    "# Delete the Kafka stream\n",
    "await streaming.delete_stream(stream)\n",
    "\n",
    "# Delete the registered dataset from the NDP endpoint system\n",
    "client.delete_resource_by_id(search_results[0][\"id\"])\n",
    "print(\"Cleanup completed: Stream and registered dataset deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d99d22-459d-4ac7-9ba9-c8b53b6ab2fc",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### **API Streams Overview**\n",
    "\n",
    "API streams enable real-time access to continuously updated data from external sources, such as sensor networks or live monitoring systems. For example, the [SAGE Continuum API](https://data.sagecontinuum.org/api/v0/stream?name=wxt.wind.direction) provides live wind direction measurements from distributed weather sensors. By registering such API endpoints with SciDX, users can discover, filter, and consume streaming data for dynamic analysis and visualization.\n",
    "\n",
    "#### **Sage Example**\n",
    "\n",
    "#### **1**. Register a API Stream from an external API\n",
    "\n",
    "In this step, we will use the `NDP endpoint client`, and the metadata for resgitsering an online **API Stream** into our NDP endpoint. Once registered, these streams can be discovered, and consumed dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291386b-45bb-4b40-a3be-51007c066772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Streaming data with the NDP endpoint client\n",
    "api_stream_metadata = {\n",
    "    \"resource_name\": \"api_stream_example_sage\",\n",
    "    \"resource_title\": \"Example API Stream\",\n",
    "    \"owner_org\": \"saleem_test\",\n",
    "    \"resource_url\": \"https://data.sagecontinuum.org/api/v0/stream?name=wxt.wind.direction\",\n",
    "    \"file_type\": \"stream\",\n",
    "    \"notes\": \"Some additional notes about the resource.\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(client.register_url(api_stream_metadata))\n",
    "    print('Correctly registered')\n",
    "except ValueError as e: # If the dataset already exists just show the error\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cdb5ef-4cd7-4d88-bf93-7d96eb6dba5d",
   "metadata": {},
   "source": [
    "#### **2**. Search for the registered entry\n",
    "\n",
    "Now that we have registered data source, we will: \n",
    "1. Use the **NDP endpoint client** to search for datasets using the `search_datasets` method.\n",
    "2. Verify that the **registered data stream** is correctly stored and available for discovery.\n",
    "3. Confirm meta data accuracy before data consumption.\n",
    "\n",
    "This ensures the dataset is discoverable for use by the Data Consumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff66712-e1d9-4fcb-b117-cf486307b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the registered Earthscope data stream\n",
    "search_results = client.search_datasets(\"api_stream_example_sage\", server=\"local\")\n",
    "print(f\"Number of datasets found: {len(search_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb26dd-5cfe-44af-a765-c45fbeea2719",
   "metadata": {},
   "source": [
    "#### **3**. Create a Data Stream from the registered entry\n",
    "\n",
    "Now we will leverage the functionalities of `Streaming Client` to consume the data stream registered by `API Client`.\n",
    "\n",
    "The `create_kafka_stream` function searches for datasets matching the provided keywords, applies filtering semantics, and creates a real-time Kafka stream for consumption.\n",
    "\n",
    "Function parameters\n",
    "- **`keywords`**: List of keywords to filter relevant datasets.\n",
    "- filter_semantics: Optional, defines filtering criteria for datasets.\n",
    "- match_all: Optional, if True only data sources with all the keywords will be selected.\n",
    "- username: Optional username for authentication in protected data sources.\n",
    "- password: Optional password for authentication in protected data sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88586b32-ab0b-4d3f-9c1d-0a4298353d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kafka stream without filters\n",
    "# Searches for datasets matching the provided keywords, applies filtering semantics, and creates a real-time Kafka stream for consumption\n",
    "stream = await streaming.create_kafka_stream(\n",
    "    keywords=[\"api_stream_example_sage\"]\n",
    ")\n",
    "\n",
    "# Retrieve the stream's topic name\n",
    "topic = stream.data_stream_id\n",
    "print(f\"Stream created: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627586af-deb9-4852-abd8-1ac91de29e31",
   "metadata": {},
   "source": [
    "#### **4**. Consume the Streamed Data \n",
    "\n",
    "Now that we have successfully created a Kafka data stream, we transition to real-time data consumption. This step involves:\n",
    "\n",
    "1. Initializing a kafka consumer: Passing the data stream topic to the consume_kafka_messages function.\n",
    "2. Listening for incoming messages: Continuously receiving new filtered data in real-time.\n",
    "3. Processing and updating the data dynamically: Messages are appended to a DataFrame for analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312f6e9-fb62-4665-b991-74e1a17170be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start consuming the filtered Kafka stream\n",
    "consumer = streaming.consume_kafka_messages(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93134c8d",
   "metadata": {},
   "source": [
    "**Note**: It may take a few seconds for data to NDP endpointulate due to real-time processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cd9d8-1da9-48a4-b86d-0a5f7ae3969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the actual state of the real-time DataFrame\n",
    "consumer.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a702b-f3c0-4f45-9b3e-5944e2b0a20b",
   "metadata": {},
   "source": [
    "#### **5**: Stop Data Consumption and Clean up \n",
    "\n",
    "To wrap up, we will: \n",
    "1. Stop the data consumer to halt data processing.\n",
    "2. Delete the created stream from the Kafka topic using the Streaming client.\n",
    "3. Remove the registered dataset using the NDP endpoint client.\n",
    "\n",
    "This ensures all resources and background tasks are properly released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db93d1-a846-49d1-b1c4-fa11acd0ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Kafka consumer\n",
    "consumer.stop()\n",
    "\n",
    "# Delete the Kafka stream\n",
    "await streaming.delete_stream(stream)\n",
    "\n",
    "# Delete the registered dataset from the NDP endpoint system\n",
    "client.delete_resource_by_id(search_results[0][\"id\"])\n",
    "print(\"Cleanup completed: Stream and registered dataset deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e13b4c-f1f6-4acd-83df-577ef54aeebe",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## **Advanced Usage**: Apply transformations and filters to the consumed data\n",
    "\n",
    "`SciDX` provides a powerful filtering system that allows users to refine their data streams by applying custom filtering conditions before consuming them. These filters enable real-time data selection based on specific rules, comparisons, and logical expressions.\n",
    "\n",
    "In this section, we will: \n",
    "1. **Register custom data streams** (via API) using the SciDX framework.\n",
    "2. **Discover and apply filters** to customize the data stream before consumption.\n",
    "3. **Consume and visualize real-time data streams**.\n",
    "\n",
    "#### **1**. Register and Pre-process a stream\n",
    "\n",
    "This time we will map the values of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbb379-a193-4539-b69c-acb0f5163618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Streaming data with the NDP endpoint client\n",
    "api_stream_metadata = {\n",
    "    \"resource_name\": \"api_stream_example_sage_advanced\",\n",
    "    \"resource_title\": \"Example API Stream\",\n",
    "    \"owner_org\": \"saleem_test\",\n",
    "    \"resource_url\": \"https://data.sagecontinuum.org/api/v0/stream\",\n",
    "    \"file_type\": \"stream\",\n",
    "    \"notes\": \"Some additional notes about the resource.\",\n",
    "    \"mapping\": {\n",
    "                \"timestamp\": \"timestamp\",\n",
    "                \"name\": \"name\",\n",
    "                \"direction\": \"value\",\n",
    "                \"vsn\": \"meta.vsn\",\n",
    "                \"sensor\": \"meta.sensor\",\n",
    "                \"units\": \"meta.units\"\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(client.register_url(api_stream_metadata))\n",
    "    print('Correctly registered')\n",
    "except ValueError as e: # If the dataset already exists just show the error\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079dfd3e",
   "metadata": {},
   "source": [
    "#### **2**. Search for the registered entry\n",
    "\n",
    "Now that we have registered data source, we will: \n",
    "1. Use the **NDP endpoint client** to search for datasets using the `search_datasets` method.\n",
    "2. Verify that the **registered data stream** is correctly stored and available for discovery.\n",
    "3. Confirm meta data accuracy before data consumption.\n",
    "\n",
    "This ensures the dataset is discoverable for use by the Data Consumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ba490-4da2-4547-9260-03004c980733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the registered Earthscope data stream\n",
    "search_results = client.search_datasets(\"api_stream_example_sage_advanced\", server=\"local\")\n",
    "print(f\"Number of datasets found: {len(search_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f220bae-9b42-4ae1-8570-2206dfc4cda2",
   "metadata": {},
   "source": [
    "### **3**. Create a Data Stream from the registered entry with Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787cf46-7578-428e-b185-4f6b1256135b",
   "metadata": {},
   "source": [
    "Now we will leverage the functionalities of `Streaming Client` to consume the data stream registered by `API Client`.\n",
    "\n",
    "The `create_kafka_stream` function searches for datasets matching the provided keywords, applies filtering semantics, and creates a real-time Kafka stream for consumption.\n",
    "\n",
    "Function parameters\n",
    "- **`keywords`**: List of keywords to filter relevant datasets.\n",
    "- **`filter_semantics`**: defines filtering criteria for datasets.\n",
    "- **`match_all`**: Optional, if True only data sources with all the keywords will be selected.\n",
    "- username: Optional username for authentication in protected data sources.\n",
    "- password: Optional password for authentication in protected data sources.\n",
    "\n",
    "The filtering capabilities allow us to refine the data stream by applying conditions, alerts, and transformations.\n",
    "\n",
    "#### Filtering capabilities: \n",
    "\n",
    "| **Type**                        | **Explanation**                                             | **Example**                                       |\n",
    "|---------------------------------|-------------------------------------------------------------|---------------------------------------------------|\n",
    "| Column Comparisons              | Column-to-column comparisons                                | `x > y`                                           |\n",
    "| Mathematical Operations         | Addition, subtraction, multiplication and division          | `x > 10*y`                                        |\n",
    "| IN Operator                     | Check if values are in a list                               | `station IN ['A', 'B']`                           |\n",
    "| Conditional Logic (IF-THEN-ELSE)| Apply rules based on conditional statements                 | `IF x > 20 THEN alert = High ELSE y = 10`         |\n",
    "| Logical Operators (AND, OR)     | Combine multiple conditions using AND and OR operators       | `IF x > 10 OR z = 20 THEN alert = High ELSE alert = Low` |\n",
    "| Window-Based Filtering          | Calculate aggregates (mean, sum, max, min) over sliding windows | `IF window_filter(9, sum, x > 20) THEN alert = High` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9dc3c-cd7f-4ac3-a9de-9182e1ca0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\n",
    "    \"name = wxt.wind.direction\"\n",
    "]\n",
    "\n",
    "# Create a Kafka stream for SAGE data\n",
    "stream = await streaming.create_kafka_stream(\n",
    "    keywords=[\"api_stream_example_sage_advanced\"],\n",
    "    match_all=True,\n",
    "    filter_semantics=filters\n",
    ")\n",
    "\n",
    "# Retrieve the stream's topic name\n",
    "topic = stream.data_stream_id\n",
    "print(f\"Stream created: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0e59d",
   "metadata": {},
   "source": [
    "`SciDX` does not remove rows from the data stream by default. Instead:\n",
    "\n",
    "If a row does not meet the filtering criteria, its affected column values are set to null.\n",
    "All rows are still sent, even if they contain null values.\n",
    "Users can later apply actions to tailor the response, like the delete_null_rows that remove rows where some columns contain null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb184e",
   "metadata": {},
   "source": [
    "#### 4. Consume the Streamed Data \n",
    "\n",
    "Now that we have successfully created a **filtered** Kafka data stream, we transition to real-time data consumption. This step involves:\n",
    "\n",
    "1. Initializing a kafka consumer: Passing the data stream topic to the consume_kafka_messages function.\n",
    "2. Listening for incoming messages: Continuously receiving new filtered data in real-time.\n",
    "3. Processing and updating the data dynamically: Messages are appended to a DataFrame for analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65fbe5-44ae-423d-8442-25f47d90a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start consuming the filtered Kafka stream\n",
    "consumer = streaming.consume_kafka_messages(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e263e",
   "metadata": {},
   "source": [
    "**Note**: It may take a few seconds for data to NDP endpointulate due to real-time processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b0cf1-0f5e-46ef-a453-48a31949d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37312881-17c0-47e6-9690-495c5d1e6d58",
   "metadata": {},
   "source": [
    "### **5**: Stop Data Consumption and Clean up \n",
    "\n",
    "To wrap up, we will: \n",
    "1. Stop the data consumer to halt data processing.\n",
    "2. Delete the created stream from the Kafka topic using the Streaming client.\n",
    "3. Remove the registered dataset using the NDP endpoint client.\n",
    "\n",
    "This ensures all resources and background tasks are properly released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1df0a-a70e-45be-8bda-c898dc1311d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Kafka consumer\n",
    "consumer.stop()\n",
    "\n",
    "# Delete the Kafka stream\n",
    "await streaming.delete_stream(stream)\n",
    "\n",
    "# Delete the registered dataset from the NDP endpoint system\n",
    "client.delete_resource_by_id(search_results[0][\"id\"])\n",
    "print(\"Cleanup completed: Stream and registered dataset deleted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
