{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b6d4f5-e7eb-40bd-82f5-723081d80802",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <div>\n",
    "        <img src=\"https://scidx.sci.utah.edu/wp-content/uploads/2024/12/logo-sm.png\" alt=\"scidx Logo\"/>\n",
    "        <img src=\"https://nationaldataplatform.org/National_Data_Platform_horiz_stacked.svg\" alt=\"NDP Logo\" width=\"400\" style=\"padding-left:100px\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb0d25",
   "metadata": {},
   "source": [
    "# SciDX Streaming Capabilities Demonstration: Kafka Stream \n",
    "\n",
    "This demonstration showcases the **SciDX Streaming capabilities**, leveraging both the **SciDX NDP Endpoint Library** for managing data objects and the **Streaming Library** for real-time data streaming and processing of a Kafka Stream.\n",
    "\n",
    "**`SciDX NDP Endpoint Library`:** Used to register and discover data objects (acts as the Data Provider). Interacts with the NDP endpoint API to register and manage data objects. <br>\n",
    "**`Streaming Library`:** Used to create, manage, and consume real-time data streams (acts as the Data Consumer). Manages real-time data streams, including applying filters and consuming messages.\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Setup**: Client preparation\n",
    "\n",
    "1. **Import necessary modules** for handling data streams:\n",
    "\n",
    "   a. Import `StreamingClient` from `scidx_streaming` module for handling streaming functionality <br>\n",
    "   b. Import `APIClient` from `pointofpresence` module for API interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scidx_streaming import StreamingClient\n",
    "from pointofpresence import APIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0216ff",
   "metadata": {},
   "source": [
    "2. **API Authentication Token Setup**\n",
    "\n",
    "    1. Navigate to https://token.ndp.utah.edu\n",
    "    \n",
    "    2. If not already authenticated:\n",
    "       - Select the `CILogon` button\n",
    "       - Choose your institution from the Identity Provider list\n",
    "       - Complete the institutional login process\n",
    "    \n",
    "    3. Upon successful authentication, you will be redirected to token.ndp.utah.edu\n",
    "    \n",
    "    4. Locate the `Access Token` field and copy the token value\n",
    "    \n",
    "    5. Replace `<your_token>` in the configuration below with your copied access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=\"<your_token>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9adbc3a",
   "metadata": {},
   "source": [
    "3. **Initialize the API or Kafka client** to register and discover data streams.\n",
    "\n",
    "    a. `APIClient`: handle data registration and discovery. <br>\n",
    "    b. `StreamingClient`: handle real-time data streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL of the API Client to connect to the NDP endpoint service\n",
    "API_URL=\"155.101.6.191:8003\"\n",
    "\n",
    "# Initialize the NDP endpoint client for data registration and discovery\n",
    "client = APIClient(base_url=API_URL, token=TOKEN)\n",
    "\n",
    "# Initialize the Streaming client for real-time data streaming\n",
    "streaming = StreamingClient(client)\n",
    "print(f\"Streaming Client initialized. User ID: {streaming.user_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ea68e",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## **Basic Usage**\n",
    "\n",
    "**Data Provider**: Register data stream\n",
    "\n",
    "1. Register the data source metadata.\n",
    "2. Verify its discoverability.\n",
    "\n",
    "> In addition to using `client.register_url(metadata)` for API-based and static file-based streams, SciDX NDP Endpoint also supports real-time data ingestion from Kafka topics. To register a Kafka-based data stream, use **`client.register_kafka_topic(metadata)`**.\n",
    "\n",
    "> **Note:**  \n",
    "> Each registration requires defining `metadata`, which includes:  \n",
    "> - **resource_name**: Unique identifier for the resource.  \n",
    "> - **resource_title**: A descriptive name for the resource.  \n",
    "> - **owner_org**: The ID of the CKAN organization registering the stream.  \n",
    "> - **resource_url**: The API endpoint, static file URL, or Kafka connection details.  \n",
    "> - **file_type**: Type of file (CSV, TXT, JSON, NetCDF, or `stream` for real-time APIs).  \n",
    "> - **notes**: Optional field for additional information.  \n",
    "> - **mapping**: Defines how fields from the stream or file should be extracted and renamed.  \n",
    "> - **processing**: Indicates whether the data requires transformations before ingestion.  \n",
    "> - **extras**: Additional metadata such as authentication parameters or custom settings.\n",
    "\n",
    "> **Additional `extras` Configuration for Kafka Streams**  \n",
    "> The following optional parameters can be included in the `extras` dictionary when registering a Kafka-based data stream using `client.register_kafka_topic(metadata)`:\n",
    "> \n",
    "> - **sasl_mechanism**:  \n",
    ">   Defines the authentication mechanism for Kafka security.  \n",
    ">   - Example: `\"SCRAM-SHA-512\"`  \n",
    ">   - Required only if connecting to secured Kafka brokers.\n",
    "> \n",
    "> - **security_protocol**:  \n",
    ">   Specifies the communication protocol used for the Kafka connection.  \n",
    ">   - Example: `\"SASL_SSL\"` (for encrypted and authenticated connections)  \n",
    ">   - Required only for secured Kafka clusters.\n",
    "> \n",
    "> - **auto_offset_reset**:  \n",
    ">   Controls where the Kafka consumer starts reading when no previous offset is available.  \n",
    ">   - `\"latest\"`: Read from the most recent message.  \n",
    ">   - `\"earliest\"`: Read from the beginning of the topic.\n",
    "> \n",
    "> - **time_window**:  \n",
    ">   Maximum time (in seconds) to wait for new messages before considering the stream inactive.  \n",
    ">   - If no messages arrive within this window, the consumer automatically stops.\n",
    "> \n",
    "> - **batch_interval**:  \n",
    ">   Time interval (in seconds) for buffering and processing messages in batches.  \n",
    ">   - Helps optimize performance for high-throughput Kafka topics.\n",
    "> \n",
    "> - **batch_mode**:  \n",
    ">   Determines how messages containing multiple records are handled.  \n",
    ">   - `\"False\"` (default): Each Kafka message is treated as a single record.  \n",
    ">   - `\"True\"`: Messages containing arrays of values will be expanded into multiple structured records.  \n",
    ">     Example:  \n",
    ">     Input message:\n",
    ">     ```json\n",
    ">     {\"id\": [1, 2, 3], \"value\": [10, 20, 30]}\n",
    ">     ```\n",
    ">     Expanded records:\n",
    ">     ```json\n",
    ">     [\n",
    ">       {\"id\": 1, \"value\": 10},\n",
    ">       {\"id\": 2, \"value\": 20},\n",
    ">       {\"id\": 3, \"value\": 30}\n",
    ">     ]\n",
    ">     ```\n",
    "\n",
    "**Data Consumer**:\n",
    "\n",
    "1. Discover and apply optional filters to registered data sources to create a custom data stream.\n",
    "2. Subscribe to and consume the custom data stream in real-time.\n",
    "3. Process and visualize the incoming data dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d99d22-459d-4ac7-9ba9-c8b53b6ab2fc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **1**. Register a **`Kafka Stream`**\n",
    "\n",
    "In this step, we will use the **NDP endpoint client**, and the metadata for resgitsering an online **Kafka Stream** into our NDP endpoint.\n",
    "\n",
    "> #### **Important:**\n",
    "> The example Kafka topic used here is hosted on the SciDX team’s Kafka server. The topic is named **`timestamp-topic-1`**, and the SciDX team continuously produces timestamped data to this topic every 5 seconds. Each message contains a UTC timestamp and a random measurement value, for example:\n",
    ">```sh\n",
    ">[17:29:13] Sent: {'timestamp': '2025-06-02T23:29:13.798342Z', 'measurement': 0.06916792150409812}\n",
    ">[17:29:18] Sent: {'timestamp': '2025-06-02T23:29:18.922484Z', 'measurement': 0.8232872454560856}\n",
    ">[17:29:23] Sent: {'timestamp': '2025-06-02T23:29:23.930366Z', 'measurement': 0.28348114450447137}\n",
    ">[17:29:28] Sent: {'timestamp': '2025-06-02T23:29:28.937124Z', 'measurement': 0.4795100980289084}\n",
    ">```\n",
    "> Please note, this is for demonstration purposes. A real-world example could involve a scientific group hosting a publicly accessible Kafka topic. Using the SciDX NDP Endpoint and Streaming libraries, we can streamline data consumption and apply filtered semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561096d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload data for the Kafka topic registration\n",
    "kafka_stream_metadata = {\n",
    "  \"dataset_name\": \"timestamp-example\",\n",
    "  \"dataset_title\": \"timestamp-example\",\n",
    "  \"owner_org\": \"saleem_test\",\n",
    "  \"kafka_topic\": \"timestamp-topic-1\",\n",
    "  \"kafka_host\": \"155.101.6.191\",\n",
    "  \"kafka_port\": \"9092\"\n",
    "}\n",
    "\n",
    "# Call the register_kafka_topic method to add the Kafka topic\n",
    "try:\n",
    "    response = client.register_kafka_topic(kafka_stream_metadata)\n",
    "    print(\"Kafka topic registered successfully with ID:\", response[\"id\"])\n",
    "except ValueError as e:\n",
    "    print(\"Failed to register Kafka topic.\")\n",
    "    print(f\"{e}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa873420",
   "metadata": {},
   "source": [
    "#### **2**. Search for the registered entry\n",
    "\n",
    "Now that we have registered data source, we will: \n",
    "1. Use the **NDP endpoint client** to search for datasets using the `search_datasets` method.\n",
    "2. Verify that the **registered data object** is correctly stored and available for discovery.\n",
    "3. Confirm meta data accuracy before data consumption.\n",
    "\n",
    "This ensures the dataset is discoverable for use by the Data Consumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff66712-e1d9-4fcb-b117-cf486307b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the registered Kafka data stream\n",
    "import json\n",
    "\n",
    "search_results = client.search_datasets(\"timestamp-example\", server=\"local\")\n",
    "print(f\"Number of datasets found: {len(search_results)}\")\n",
    "print(\"Search result:\\n\" + json.dumps(search_results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7831427",
   "metadata": {},
   "source": [
    "#### **3**. Create a Data Stream from the registered entry\n",
    "\n",
    "Now we will leverage the functionalities of `Streaming Client` to create a data stream with the metadata registered by `API Client`.\n",
    "\n",
    "The `create_kafka_stream` function searches for datasets matching the provided keywords, applies filtering semantics, and creates a real-time Kafka stream for consumption.\n",
    "\n",
    "Function parameters\n",
    "- **`keywords`**: List of keywords to filter relevant datasets.\n",
    "- filter_semantics: Optional, defines filtering criteria for datasets.\n",
    "- match_all: Optional, if True only data sources with all the keywords will be selected.\n",
    "- username: Optional username for authentication in protected data sources.\n",
    "- password: Optional password for authentication in protected data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b63b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kafka stream data\n",
    "stream = await streaming.create_kafka_stream(\n",
    "    keywords=[\"timestamp-example\"],\n",
    "    match_all=True\n",
    ")\n",
    "\n",
    "# Retrieve the stream's topic name\n",
    "topic = stream.data_stream_id\n",
    "print(f\"Stream created: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67616533",
   "metadata": {},
   "source": [
    "#### **4**. Consume the Streamed Data \n",
    "\n",
    "Now that we have successfully created a Kafka data stream, we transition to real-time data consumption. This step involves:\n",
    "\n",
    "1. Initializing a kafka consumer: Passing the data stream topic to the consume_kafka_messages function.\n",
    "2. Listening for incoming messages: Continuously receiving new filtered data in real-time.\n",
    "3. Processing and updating the data dynamically: Messages are appended to a DataFrame for analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312f6e9-fb62-4665-b991-74e1a17170be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start consuming the filtered Kafka stream\n",
    "consumer = streaming.consume_kafka_messages(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371f82c",
   "metadata": {},
   "source": [
    "**Note**: It may take a few seconds for data to NDP endpointulate due to real-time processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cd9d8-1da9-48a4-b86d-0a5f7ae3969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After some seconds you can visualize the dataset\n",
    "consumer.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82fdaed",
   "metadata": {},
   "source": [
    "#### **5**. Stop Data Consumption and Clean up \n",
    "\n",
    "To wrap up, we will: \n",
    "1. Stop the data consumer to halt data processing.\n",
    "2. Delete the created stream from the Kafka topic using the Streaming client.\n",
    "3. Remove the registered dataset using the NDP endpoint client.\n",
    "\n",
    "This ensures all resources and background tasks are properly released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db93d1-a846-49d1-b1c4-fa11acd0ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Kafka consumer\n",
    "consumer.stop()\n",
    "\n",
    "# Delete the Kafka stream - this will cause error\n",
    "await streaming.delete_stream(stream)\n",
    "\n",
    "# Delete the registered dataset from the NDP endpoint system\n",
    "client.delete_resource_by_id(search_results[0][\"id\"])\n",
    "print(\"Cleanup completed: Stream and registered dataset deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891a105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
